{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fake_news_detection_Ana_Resende_10562563",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-UxOxFXtwv2"
      },
      "source": [
        "#installing ekphrasis\n",
        "!pip install ekphrasis\n",
        "#installing tweet-preprocessor\n",
        "!pip install tweet-preprocessor\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG2o7WLvUJRm"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import preprocessor as p\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from numpy import set_printoptions\n",
        "from pandas import DataFrame\n",
        "from sklearn import metrics\n",
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "\n",
        "#Single Models\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#Ensemble Models\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "#Vectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0gy5KodH5hu"
      },
      "source": [
        "#Libraries for preprocessing using NLTK\n",
        "import nltk\n",
        "from nltk import word_tokenize, FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import re\n",
        "re.compile('<title>(.*)</title>')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-cLuqshVDZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e43b10b-a4ae-4404-a74a-010b6668a7e5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount = True)\n",
        "val = pd.read_csv('/content/drive/My Drive/fake_news/Val.csv',encoding = \"ISO-8859-1\")\n",
        "train = pd.read_csv('/content/drive/My Drive/fake_news/Train.csv',encoding = \"ISO-8859-1\")\n",
        "test = pd.read_csv('/content/drive/My Drive/fake_news/test_with_labels.csv',encoding = \"ISO-8859-1\")\n",
        "\n",
        "frames = [val, train, test]\n",
        "\n",
        "df = pd.concat(frames)\n",
        "df = df.reset_index(drop=True)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Dqe-xdrHbG_"
      },
      "source": [
        "for i,v in enumerate(df['tweet']):\n",
        "     df.loc[i,'tweet'] = p.clean(v)\n",
        "\n",
        "def preprocess_data(data):\n",
        " data = data.astype(str).str.replace('\\d+', '')\n",
        " lower_text = data.str.lower()\n",
        " lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        " w_tokenizer =  TweetTokenizer()\n",
        " \n",
        " def lemmatize_text(text):\n",
        "  return [(lemmatizer.lemmatize(w)) for w \\\n",
        "                       in w_tokenizer.tokenize((text))]\n",
        " def remove_punctuation(words):\n",
        "   new_words = []\n",
        "   for word in words:\n",
        "      new_word = re.sub(r'[^\\w\\s]', '', (word))\n",
        "      if new_word != '':\n",
        "         new_words.append(new_word)\n",
        "   return new_words\n",
        " words = lower_text.apply(lemmatize_text)\n",
        " words = words.apply(remove_punctuation)\n",
        " return pd.DataFrame(words)\n",
        "pre_tweets = preprocess_data(df['tweet'])\n",
        "df['tweet'] = pre_tweets\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['tweet'] = df['tweet'] .apply(lambda x: [item for item in \\\n",
        "                                    x if item not in stop_words])\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzOiBS4diMUH"
      },
      "source": [
        "df['tweet'] = df['tweet'].astype(str).str.replace('[', '')\n",
        "df['tweet'] = df['tweet'].astype(str).str.replace(']', '')\n",
        "features = df['tweet']\n",
        "target = df['label'].map({'real': 1, 'fake':0})\n",
        "\n",
        "trainx, testx, trainy, testy = train_test_split(features, target, test_size = 0.20, random_state = 50)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVaNaNcMq4pQ"
      },
      "source": [
        "def TFIDF_verctorizer(trainx,testx):\n",
        "    vectorizer =  TfidfVectorizer(max_features=5)\n",
        "    trainx_con = vectorizer.fit_transform(trainx)\n",
        "    testx_con = vectorizer.fit_transform(testx)\n",
        "    return trainx_con,testx_con\n",
        "    \n",
        "trainx_con_TF_IDF,testx_con_TF_IDF=TFIDF_verctorizer(trainx,testx)\n",
        "trainx_con_TF_IDF = trainx_con_TF_IDF.toarray()\n",
        "testx_con_TF_IDF = testx_con_TF_IDF.toarray()\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "affPGrlXBza5"
      },
      "source": [
        "names = []\n",
        "accuracy_final = []\n",
        "\n",
        "\n",
        "dict_classifiers = {\n",
        "    \"Logreg\": LogisticRegression(),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"SVM\": SVC(probability=True, kernel='linear'),\n",
        "    \"DT\": DecisionTreeClassifier(),\n",
        "    \"NB\": MultinomialNB(),\n",
        "}\n",
        "\n",
        "for name, model in dict_classifiers.items():\n",
        "    model = model.fit(trainx_con_TF_IDF, trainy)\n",
        "    result = model.predict(testx_con_TF_IDF)\n",
        "    accuracy = accuracy_score(testy,result)*100\n",
        "    names.append(name)\n",
        "    accuracy_final.append(accuracy)\n",
        "     \n",
        "      \n",
        "result_sm = pd.DataFrame(names, accuracy_final)\n",
        "result_sm\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ldm8QomBzQx"
      },
      "source": [
        "DT = DecisionTreeClassifier()\n",
        "NB = MultinomialNB()\n",
        "\n",
        "names = []\n",
        "accuracy_final = []\n",
        "\n",
        "\n",
        "dict_classifiers = {\n",
        "    \"RD\": RandomForestClassifier ( n_estimators=100,max_depth=10, min_samples_leaf=1, min_samples_split= 2, random_state=50),\n",
        "    \"ET\": ExtraTreesClassifier( n_estimators=100,max_depth=10,min_samples_leaf=1, random_state=10),\n",
        "    \"AdaB\": AdaBoostClassifier(base_estimator = DT,n_estimators= 100,learning_rate=1,  random_state=40),\n",
        "    \"GB\": GradientBoostingClassifier(n_estimators= 200,max_depth=5,  random_state=80),\n",
        "    \"XGBC\":XGBClassifier(base_estimator = DT,n_estimators= 200,max_depth=10,subsample=0.8,  random_state=90)\n",
        "    }\n",
        "\n",
        "for name, model in dict_classifiers.items():\n",
        "     model = model.fit(trainx_con_TF_IDF, trainy)\n",
        "     result = model.predict(testx_con_TF_IDF)\n",
        "     accuracy = accuracy_score(testy,result)*100\n",
        "     names.append(name)\n",
        "     accuracy_final.append(accuracy)\n",
        "     \n",
        "      \n",
        "result_em = pd.DataFrame(names, accuracy_final)\n",
        "result_em\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}